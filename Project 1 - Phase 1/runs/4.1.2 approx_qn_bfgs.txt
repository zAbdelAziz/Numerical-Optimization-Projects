###################
# Approx Solutions #
###################
Running: Rosenbrock
===================
	- Starting Point : [1.2 1.2]
	------------------------------
		- Quasi Newton:
			- BFGS Method:
			---------------
			Iteration 0:
				X = [1.2, 1.2]
				Gradient = [115.59999998800308, -47.9999999960512]
				Hessian = [[0.153723421134914, 0.3597772164457445], [0.3597772164457445, 0.8471310808415595]]
				Gradient Difference = [-145.47317582741525, 61.8386839403573]
				|| Gradient || = 125.16932530315574
				||X optimum - X|| = 0.28284271247461895
				Rho = 0.051035845674458
				alpha = [-0.11448469  0.0475369 ]
			Iteration 1:
				X = [1.085515305178769, 1.247536897504648]
				Gradient = [-29.87317583941218, 13.838683944306096]
				Hessian = [[0.056315674835814095, 0.12220565486617094], [0.12220565486617135, 0.26839713710292534]]
				Gradient Difference = [43.73307408640781, -21.773259337262708]
				|| Gradient || = 32.92287666718744
				||X optimum - X|| = 0.26189193008957823
				Rho = 0.45098215862173735
				alpha = [-0.19795784 -0.49945151]
			Iteration 2:
				X = [0.8875574691183186, 0.7480853840240573]
				Gradient = [13.859898246995629, -7.934575392956611]
				Hessian = [[0.03568631403529385, 0.07444478319670991], [0.07444478319671, 0.15782397134704387]]
				Gradient Difference = [-14.768148313774855, 8.380042585768242]
				|| Gradient || = 15.970418469269521
				||X optimum - X|| = 0.27587007176093703
				Rho = 2.2722350548066474
				alpha = [0.09682968 0.22316   ]
			Iteration 3:
				X = [0.9843871441498668, 0.9712453855315246]
				Gradient = [-0.9082500667792265, 0.4454671928116319]
				Hessian = [[0.03337573297534476, 0.06863696065554718], [0.06863696065554713, 0.1458778594194223]]
				Gradient Difference = [0.47670377643503803, -0.24274019822491325]
				|| Gradient || = 1.0116121804703346
				||X optimum - X|| = 0.03271985820610614
				Rho = 3385.5194182956816
				alpha = [-0.00075061 -0.00269092]
			Iteration 4:
				X = [0.9836365326654181, 0.9685544633669505]
				Gradient = [-0.43154629034418845, 0.20272699458671864]
				Hessian = [[0.03919341814670384, 0.07463883342317919], [0.07463883342317919, 0.14697079220305723]]
				Gradient Difference = [0.3608981854111954, -0.18296406657401443]
				|| Gradient || = 0.4767918152023942
				||X optimum - X|| = 0.03544834040333042
				Rho = 5959.347400102264
				alpha = [4.88609002e-04 4.66457346e-05]
			Iteration 5:
				X = [0.9841251416671803, 0.9686011091015085]
				Gradient = [-0.07064810493299305, 0.01976292801270422]
				Hessian = [[0.2842150960315536, 0.5442090048280751], [0.5442090048280759, 1.0450692325068485]]
				Gradient Difference = [0.07340520024054574, -0.035958624418109014]
				|| Gradient || = 0.07336026209235165
				||X optimum - X|| = 0.035183824077869415
				Rho = 101965.04095129562
				alpha = [0.00129386 0.00236852]
			Iteration 6:
				X = [0.9854190004931969, 0.9709696280509864]
				Gradient = [0.00275709530755269, -0.016195696405404796]
				Hessian = [[0.40963998632119913, 0.7994725533525628], [0.7994725533525594, 1.5629405196454287]]
				Gradient Difference = [0.2013367815636763, -0.09311809444593891]
				|| Gradient || = 0.016428699175253372
				||X optimum - X|| = 0.03248642858357115
				Rho = 5542.693159454878
				alpha = [0.00803024 0.01542519]
			Iteration 7:
				X = [0.9934492362088756, 0.9863948159697573]
				Gradient = [0.20409387687122899, -0.10931379085134371]
				Hessian = [[0.4511449901305925, 0.8996744581910008], [0.8996744581909982, 1.7986642604797034]]
				Gradient Difference = [-0.04826076317998698, 0.028411347965493244]
				|| Gradient || = 0.23152497801882996
				||X optimum - X|| = 0.015100117176494933
				Rho = 28193.37171333579
				alpha = [0.00378836 0.0076835 ]
			Iteration 8:
				X = [0.9972375987676766, 0.9940783161815039]
				Gradient = [0.155833113691242, -0.08090244288585047]
				Hessian = [[0.4640728890360283, 0.9277743981669551], [0.9277743981669645, 1.8597289758147122]]
				Gradient Difference = [-0.13849183441314963, 0.07194942948123717]
				|| Gradient || = 0.17558235841793973
				||X optimum - X|| = 0.006534309436702597
				Rho = 25797.585243815913
				alpha = [0.00248253 0.00531726]
			Iteration 9:
				X = [0.9997201316990776, 0.9993955766573992]
				Gradient = [0.01734127927809238, -0.0089530134046133]
				Hessian = [[0.4792446723078714, 0.9594386284541365], [0.9594386284541354, 1.9257105241866106]]
				Gradient Difference = [-0.017054792009219052, 0.008788657248878947]
				|| Gradient || = 0.019516055339744803
				||X optimum - X|| = 0.0006660734516117012
				Rho = 1920397.8315915337
				alpha = [0.00025876 0.00056138]
			Iteration 10:
				X = [0.9999788907481584, 0.9999569601611387]
				Gradient = [0.00028648726887332983, -0.00016435615573435423]
				Hessian = [[0.4976652196145502, 0.9958692330767124], [0.9958692330767098, 1.9976843971728218]]
				Gradient Difference = [-0.000299839515094431, 0.0001703153786052473]
				|| Gradient || = 0.0003302845760163119
				||X optimum - X|| = 4.793775383268203e-05
				Rho = 1023770040.752504
				alpha = [2.03921473e-05 4.16354265e-05]
			Iteration 11:
				X = [0.9999992828955026, 0.9999985955876337]
				Gradient = [-1.3352246221101193e-05, 5.959222870893064e-06]
				Hessian = [[0.49891170758009185, 0.9975489036364252], [0.997548903636423, 1.9994798221024683]]
				Gradient Difference = [1.2716077480266143e-05, -5.647701151352576e-06]
				|| Gradient || = 1.4621724124530802e-05
				||X optimum - X|| = 1.5768997922670786e-06
				Rho = 855687775479.8203
				alpha = [7.10341838e-07 1.39244466e-06]
			Iteration 12:
				X = [0.9999999932373403, 0.9999999880322893]
				Gradient = [-6.361687408350495e-07, 3.1152171954048724e-07]
				Hessian = [[0.49663293383880147, 0.9931207200114262], [0.9931207200114255, 1.9909448908463534]]
				Gradient Difference = [6.291591219506056e-07, -3.07945702280495e-07]
				|| Gradient || = 7.083476890349218e-07
				||X optimum - X|| = 1.3746260046734159e-08
				Rho = 1778823750235251.2
				alpha = [6.63388301e-09 1.17280376e-08]
	- Starting Point : [-1.2  1. ]
	------------------------------
		- Quasi Newton:
			- BFGS Method:
			---------------
			Iteration 0:
				X = [-1.2, 1.0]
				Gradient = [-215.59999997755597, -87.9999999980896]
				Hessian = [[0.157911105311667, -0.3637910601751575], [-0.3637910601751575, 0.8433174010532202]]
				Gradient Difference = [256.61368404694684, 110.80159740206597]
				|| Gradient || = 232.86768773272496
				||X optimum - X|| = 2.2
				Rho = 0.015516241982425308
				alpha = [0.2135199  0.08715098]
			Iteration 1:
				X = [-0.9864801020462848, 1.087150978763799]
				Gradient = [41.01368406939088, 22.80159740397636]
				Hessian = [[0.02244418225935155, -0.04469058320874226], [-0.044690583208741745, 0.09168262315851583]]
				Gradient Difference = [-71.63046361347014, -42.80053877891987]
				|| Gradient || = 46.925847090029485
				||X optimum - X|| = 1.9883909296024527
				Rho = 0.11007496861092173
				alpha = [ 0.30509386 -0.72285847]
			Iteration 2:
				X = [-0.6813862430305399, 0.3642925053131808]
				Gradient = [-30.616779544079264, -19.998941374943513]
				Hessian = [[0.011468475860816334, -0.021362829033618545], [-0.02136282903361856, 0.0422780262642848]]
				Gradient Difference = [21.65403618792361, 16.576260882583682]
				|| Gradient || = 36.569725809324545
				||X optimum - X|| = 1.7975493642883207
				Rho = 0.603027526614495
				alpha = [-0.10577704  0.23822012]
			Iteration 3:
				X = [-0.7871632789713885, 0.6025126253002888]
				Gradient = [-8.962743356155656, -3.422680492359831]
				Hessian = [[0.10993203643263261, -0.17093061756305608], [-0.17093061756305608, 0.26915271804833185]]
				Gradient Difference = [-0.020581055304447204, -0.18682074287568184]
				|| Gradient || = 9.594035137578603
				||X optimum - X|| = 1.8308328156195566
				Rho = 123.06049073906196
				alpha = [ 0.02967087 -0.04676538]
			Iteration 4:
				X = [-0.7574924113396098, 0.555747247060779]
				Gradient = [-8.983324411460103, -3.6095012352355127]
				Hessian = [[0.05001213766556529, -0.07048623331129576], [-0.07048623331129575, 0.10220379683487375]]
				Gradient Difference = [-6.728135417244374, -7.465654157989832]
				|| Gradient || = 9.681354071032713
				||X optimum - X|| = 1.8127713822791864
				Rho = 1.1372264751046197
				alpha = [ 0.18973741 -0.28877728]
			Iteration 5:
				X = [-0.5677550052578844, 0.26696996902803494]
				Gradient = [-15.711459828704477, -11.075155393225344]
				Hessian = [[0.054783322235184444, -0.061919388903596904], [-0.061919388903596904, 0.07498488646445786]]
				Gradient Difference = [6.935179022837801, 6.053264557825955]
				|| Gradient || = 19.222617848073735
				||X optimum - X|| = 1.730661371504521
				Rho = 5.444131216763448
				alpha = [0.0051177  0.02448131]
			Iteration 6:
				X = [-0.5626373003834544, 0.2914512776047799]
				Gradient = [-8.776280805866676, -5.02189083539939]
				Hessian = [[0.07735068112235115, -0.07343869663062708], [-0.07343869663062708, 0.07583756343562634]]
				Gradient Difference = [1.32531615770759, -0.9167823413758214]
				|| Gradient || = 10.111502971670047
				||X optimum - X|| = 1.7157728942251036
				Rho = 2.645053238448487
				alpha = [ 0.16984141 -0.16685603]
			Iteration 7:
				X = [-0.3927958926377, 0.124595247388473]
				Gradient = [-7.450964648159086, -5.938673176775211]
				Hessian = [[0.06811488544361001, -0.04047656561539143], [-0.04047656561539143, 0.03472021919996546]]
				Gradient Difference = [1.306552124824556, -1.2652537004775866]
				|| Gradient || = 9.528101242570097
				||X optimum - X|| = 1.645057409163369
				Rho = 3.2713399465883186
				alpha = [ 0.14020877 -0.09681463]
			Iteration 8:
				X = [-0.25258711990167226, 0.027780618753933506]
				Gradient = [-6.14441252333453, -7.203926877252798]
				Hessian = [[0.18630533165239985, -0.07322959420062829], [-0.07322959420062829, 0.03397412896961683]]
				Gradient Difference = [4.567081268502804, 9.885837305323442]
				|| Gradient || = 9.468387809427508
				||X optimum - X|| = 1.585618181724103
				Rho = 1.6842500477911382
				alpha = [0.12693574 0.0014172 ]
			Iteration 9:
				X = [-0.12565138369206277, 0.02919782236389036]
				Gradient = [-1.5773312548317264, 2.681910428070644]
				Hessian = [[0.061814157745789675, -0.013931032043024736], [-0.013931032043024734, 0.006103432473922842]]
				Gradient Difference = [0.20354473817540963, -10.628544410251628]
				|| Gradient || = 3.1113690606649502
				||X optimum - X|| = 1.4864548111903262
				Rho = 1.3292261543622776
				alpha = [ 0.16064854 -0.06770619]
			Iteration 10:
				X = [0.03499715561176431, -0.038508369009881235]
				Gradient = [-1.3737865166563168, -7.946633982180984]
				Hessian = [[0.0393104444448119, -0.0005294120065147709], [-0.0005294120065147717, 0.004763352771990581]]
				Gradient Difference = [-0.5737808557726432, 6.100684714793747]
				|| Gradient || = 8.06450748899772
				||X optimum - X|| = 1.4176495061124768
				Rho = 5.156433910063127
				alpha = [-0.02578536  0.02936348]
			Iteration 11:
				X = [0.00921179942144424, -0.009144889088473513]
				Gradient = [-1.94756737242896, -1.8459492673872369]
				Hessian = [[0.17986412154761489, 0.016823450294917282], [0.016823450294917282, 0.006845133398028395]]
				Gradient Difference = [0.40793634192848316, 0.13132618242872596]
				|| Gradient || = 2.683383567050622
				||X optimum - X|| = 1.4142258184530079
				Rho = 31.39503531894891
				alpha = [0.07558247 0.00776184]
			Iteration 12:
				X = [0.08479427071226922, -0.001383047079170854]
				Gradient = [-1.5396310305004768, -1.714623084958511]
				Hessian = [[0.07222788169699858, 0.019895901476963292], [0.019895901476963292, 0.009232478832789945]]
				Gradient Difference = [3.918775218325621, -6.357613922536309]
				|| Gradient || = 2.304429654719927
				||X optimum - X|| = 1.3566022017889594
				Rho = 2.0367277860342283
				alpha = [0.15655437 0.01927103]
			Iteration 13:
				X = [0.24134864334726935, 0.017887982608133866]
				Gradient = [2.379144187825144, -8.07223700749482]
				Hessian = [[0.06293886012450174, 0.029211577292185903], [0.029211577292185903, 0.018545176195546766]]
				Gradient Difference = [-3.1943173011694803, 6.497785969394876]
				|| Gradient || = 8.415541418805512
				||X optimum - X|| = 1.2410060014587156
				Rho = 4.70418863343578
				alpha = [-0.01123611  0.02719154]
			Iteration 14:
				X = [0.23011253060874703, 0.04507952155263453]
				Gradient = [-0.8151731133443363, -1.5744510380999444]
				Hessian = [[0.185405583172306, 0.10353874047279998], [0.1035387404728, 0.06318351625838917]]
				Gradient Difference = [0.6627316762319868, -0.24701757092571164]
				|| Gradient || = 1.7729645445111109
				||X optimum - X|| = 1.2266213089963067
				Rho = 19.45979513216377
				alpha = [0.09729826 0.05301096]
			Iteration 15:
				X = [0.3274107953589997, 0.09809048587242941]
				Gradient = [-0.15244143711234948, -1.821468609025656]
				Hessian = [[0.09756848125004433, 0.07171580288416259], [0.07171580288416257, 0.056764140291021975]]
				Gradient Difference = [3.7880708232684857, -3.605425923119965]
				|| Gradient || = 1.8278365040163567
				||X optimum - X|| = 1.1250853344850975
				Rho = 5.586380399094259
				alpha = [0.1110303  0.06700564]
			Iteration 16:
				X = [0.4384410976369896, 0.16509612343633048]
				Gradient = [3.635629386156136, -5.426894532145621]
				Hessian = [[0.15553992535921168, 0.1362850106053681], [0.13628501060536813, 0.12479460906494408]]
				Gradient Difference = [-2.565661433945321, 3.1810838694024124]
				|| Gradient || = 6.5321501281365295
				||X optimum - X|| = 1.0061873006176296
				Rho = 16.10559151787897
				alpha = [0.03447126 0.04732092]
			Iteration 17:
				X = [0.4729123585821995, 0.21241704558618213]
				Gradient = [1.0699679522108152, -2.2458106627432084]
				Hessian = [[0.10090961246172246, 0.10957400699426459], [0.10957400699426464, 0.12399826516249293]]
				Gradient Difference = [5.106036515711843, -3.4278254647523987]
				|| Gradient || = 2.4876689795166267
				||X optimum - X|| = 0.9476857558381775
				Rho = 3.9652086344485533
				alpha = [0.13964759 0.13444447]
			Iteration 18:
				X = [0.6125599531481877, 0.3468615155626489]
				Gradient = [6.176004467922658, -5.673636127495607]
				Hessian = [[0.09943379283999121, 0.12160061691574088], [0.12160061691574087, 0.15370892676318612]]
				Gradient Difference = [-7.027696665176664, 5.733978817951124]
				|| Gradient || = 8.386487828347692
				||X optimum - X|| = 0.7594074464723496
				Rho = 6.082336290841887
				alpha = [-0.00153517  0.02679148]
			Iteration 19:
				X = [0.6110247804453617, 0.37365299577058175]
				Gradient = [-0.8516921972540059, 0.0603426904555171]
				Hessian = [[0.29283797082112584, 0.37518903329114506], [0.37518903329114506, 0.48456748839776553]]
				Gradient Difference = [1.8570675070356257, -1.243294877856238]
				|| Gradient || = 0.8538271717126171
				||X optimum - X|| = 0.7373006789192221
				Rho = 37.86271851491989
				alpha = [0.07734928 0.09429109]
			Iteration 20:
				X = [0.688374057564807, 0.46794408219107103]
				Gradient = [1.0053753097816198, -1.182952187400721]
				Hessian = [[0.12487815141639082, 0.1749512632231303], [0.17495126322313018, 0.2482540549968385]]
				Gradient Difference = [3.6436800040853816, -2.163537900959356]
				|| Gradient || = 1.552467516953137
				||X optimum - X|| = 0.6165989196180318
				Rho = 16.22882856717587
				alpha = [0.07650233 0.10035936]
			Iteration 21:
				X = [0.7648763920238978, 0.5683034446341604]
				Gradient = [4.649055313867001, -3.3464900883600768]
				Hessian = [[0.14165867883123023, 0.21751893212993467], [0.2175189321299347, 0.3390013192527027]]
				Gradient Difference = [-3.00282023804957, 1.97813945834735]
				|| Gradient || = 5.728238055710258
				||X optimum - X|| = 0.49157402997151023
				Rho = 50.69225764034895
				alpha = [0.00490723 0.01742163]
			Iteration 22:
				X = [0.7697836269179184, 0.5857250791210399]
				Gradient = [1.6462350758174316, -1.3683506300127268]
				Hessian = [[0.23867615950364637, 0.38087675884390904], [0.3808767588439089, 0.613890483558797]]
				Gradient Difference = [-0.5040838872184816, 0.4850689466154995]
				|| Gradient || = 2.1406712432103747
				||X optimum - X|| = 0.47394439389482906
				Rho = 53.10477549104491
				alpha = [0.06443868 0.10578537]
			Iteration 23:
				X = [0.8342223088516825, 0.6915104521689713]
				Gradient = [1.14215118859895, -0.8832816833972273]
				Hessian = [[0.23517129359507022, 0.41143803859863187], [0.41143803859863265, 0.7264759098525319]]
				Gradient Difference = [1.4361046050126447, -0.6657463495120947]
				|| Gradient || = 1.44384759231819
				||X optimum - X|| = 0.3502114275740577
				Rho = 49.34062703578798
				alpha = [0.06381721 0.10721938]
			Iteration 24:
				X = [0.898039514302888, 0.7987298290850373]
				Gradient = [2.5782557936115946, -1.549028032909322]
				Hessian = [[0.3717350311533105, 0.6731602084508044], [0.6731602084508039, 1.2243170441490598]]
				Gradient Difference = [-2.7794396905409835, 1.580919953354104]
				|| Gradient || = 3.0078049777255638
				||X optimum - X|| = 0.22562274340972205
				Rho = 62.9884766964336
				alpha = [0.03099731 0.06453904]
			Iteration 25:
				X = [0.9290368196947734, 0.8632688719507993]
				Gradient = [-0.201183896929389, 0.03189192044478206]
				Hessian = [[0.3167184632713428, 0.5947226675327664], [0.5947226675327711, 1.120515095341014]]
				Gradient Difference = [1.625251775469721, -0.7938023657511148]
				|| Gradient || = 0.20369598664026578
				||X optimum - X|| = 0.1540492594485252
				Rho = 123.18704293848587
				alpha = [0.04265498 0.07710654]
			Iteration 26:
				X = [0.9716918039972413, 0.9403754097289913]
				Gradient = [1.424067878540332, -0.7619104453063328]
				Hessian = [[0.3495800407167991, 0.6799428701987008], [0.6799428701987011, 1.3275094720302185]]
				Gradient Difference = [-1.0549681763858039, 0.5454761814264997]
				|| Gradient || = 1.6150779700550857
				||X optimum - X|| = 0.06600337662511119
				Rho = 666.304151201
				alpha = [0.00209682 0.00680671]
			Iteration 27:
				X = [0.9737886263655262, 0.9471821175194904]
				Gradient = [0.36909970215452803, -0.21643426387983308]
				Hessian = [[0.4416919901583229, 0.8666907022928394], [0.8666907022928427, 1.706074809905233]]
				Gradient Difference = [-0.2366024280307214, 0.14150197143317422]
				|| Gradient || = 0.42787659519044335
				||X optimum - X|| = 0.058964097699624865
				Rho = 1171.6019680109312
				alpha = [0.01813305 0.03635182]
			Iteration 28:
				X = [0.9919216720495846, 0.9835339420194211]
				Gradient = [0.13249727412380663, -0.07493229244665885]
				Hessian = [[0.4901081032531687, 0.9710409344415212], [0.9710409344415232, 1.930167043329418]]
				Gradient Difference = [-0.0773173827360617, 0.04563557590233533]
				|| Gradient || = 0.15221818584371175
				||X optimum - X|| = 0.01834095002703644
				Rho = 10293.841767347407
				alpha = [0.00642014 0.01300594]
			Iteration 29:
				X = [0.9983418085162947, 0.9965398830488686]
				Gradient = [0.05517989138774492, -0.02929671654432352]
				Hessian = [[0.5720594486459118, 1.136028964228436], [1.1360289642284205, 2.2615294183858747]]
				Gradient Difference = [-0.06105721291917422, 0.03198206718838353]
				|| Gradient || = 0.06247493908633898
				||X optimum - X|| = 0.0038369269359917746
				Rho = 109766.05031974446
				alpha = [0.0014042  0.00296562]
			Iteration 30:
				X = [0.9997460076197995, 0.9995055065049481]
				Gradient = [-0.005877321531429299, 0.0026853506440600037]
				Hessian = [[0.4991201160272192, 0.9979376756550058], [0.9979376756550082, 2.000237202652668]]
				Gradient Difference = [0.00830712997978987, -0.0038426392438048983]
				|| Gradient || = 0.006461734787605873
				||X optimum - X|| = 0.0005559100159635518
				Rho = 3734172.96577998
				alpha = [0.00031154 0.00060381]
			Iteration 31:
				X = [1.000057548823822, 1.0001093145165119]
				Gradient = [0.002429808448360571, -0.0011572885997448944]
				Hessian = [[0.49708409496288686, 0.993933810626938], [0.9939338106269244, 1.9923725358536113]]
				Gradient Difference = [-0.0024304166380599854, 0.0011572773390303116]
				|| Gradient || = 0.0026913353560684156
				||X optimum - X|| = 0.00012353756774158997
				Rho = 74644123.0972028
				alpha = [-5.78643794e-05 -1.09945684e-04]
			Iteration 32:
				X = [0.9999996844444438, 0.9999993688326836]
				Gradient = [-6.081896994143403e-07, -1.1260714582718505e-08]
				Hessian = [[0.5000000025584167, 0.9999990718820547], [0.9999990718820547, 2.0049698135035983]]
				Gradient Difference = [6.63715144754132e-07, -1.834375984558736e-08]
				|| Gradient || = 6.082939373087809e-07
				||X optimum - X|| = 7.056539452030539e-07
				Rho = 5086896468063.109
				alpha = [3.13513831e-07 6.26935844e-07]
	- Starting Point : [0.2 0.8]
	------------------------------
		- Quasi Newton:
			- BFGS Method:
			---------------
			Iteration 0:
				X = [0.2, 0.8]
				Gradient = [-62.400000000906175, 152.00000000348268]
				Hessian = [[0.8011230026581653, 0.4045814546061569], [0.4045814546061569, 0.20901796343418816]]
				Gradient Difference = [93.54400635519511, -184.5007444942759]
				|| Gradient || = 164.309951010801
				||X optimum - X|| = 0.8246211251235323
				Rho = 0.006250010833745554
				alpha = [ 0.29467567 -0.71779971]
			Iteration 1:
				X = [0.49467566853534556, 0.08220029458736644]
				Gradient = [31.144006354288933, -32.50074449079321]
				Hessian = [[0.017707117052246372, 0.012133154233752353], [0.012133154233752368, 0.012484477822721596]]
				Gradient Difference = [-23.43901802304149, 12.305370523657544]
				|| Gradient || = 45.013859246367836
				||X optimum - X|| = 1.0477160775829093
				Rho = 0.21647519719320898
				alpha = [-0.26573448 -0.1307631 ]
			Iteration 2:
				X = [0.22894119127864443, -0.04856280077092287]
				Gradient = [7.704988331247442, -20.195373967135666]
				Hessian = [[0.019637648845599376, 0.014193777456192545], [0.014193777456192547, 0.014385171343341819]]
				Gradient Difference = [-8.509513242160871, 19.424518288485572]
				|| Gradient || = 21.6152717969754
				||X optimum - X|| = 1.3015435581137338
				Rho = 0.4635156137949393
				alpha = [0.10860046 0.15864289]
			Iteration 3:
				X = [0.33754164816261767, 0.11008008585106391]
				Gradient = [-0.8045249109134289, -0.7708556786500953]
				Hessian = [[0.22569395528405833, 0.17160360605241162], [0.17160360605241162, 0.13338549376246972]]
				Gradient Difference = [-0.45042570892550593, 0.748228399183315]
				|| Gradient || = 1.114216679819217
				||X optimum - X|| = 1.1094181004102786
				Rho = 208.47685018146237
				alpha = [0.02674033 0.02250814]
			Iteration 4:
				X = [0.36428197980427446, 0.1325882244129019]
				Gradient = [-1.2549506198389349, -0.022627279466780337]
				Hessian = [[0.09969879662960271, 0.08304740489839849], [0.08304740489839833, 0.07156307511854128]]
				Gradient Difference = [6.488703432783183, -5.577062904382757]
				|| Gradient || = 1.2551545928730032
				||X optimum - X|| = 1.0754257713244253
				Rho = 2.421932242237987
				alpha = [0.18375532 0.13975821]
			Iteration 5:
				X = [0.5480373025750169, 0.2723464340937008]
				Gradient = [5.233752812944248, -5.599690183849537]
				Hessian = [[0.04577957065249631, 0.041653171878291546], [0.041653171878291546, 0.04134486183459595]]
				Gradient Difference = [-4.735616987217028, 4.114625832274399]
				|| Gradient || = 7.664769967983559
				||X optimum - X|| = 0.8565920801874249
				Rho = 9.672874990851374
				alpha = [-0.0454073  -0.02713483]
			Iteration 6:
				X = [0.5026300071321371, 0.24521160231183714]
				Gradient = [0.4981358257272195, -1.4850643515751383]
				Hessian = [[0.20011725404330633, 0.20888028668636224], [0.20888028668636224, 0.22225771090106333]]
				Gradient Difference = [0.2229008642801178, -0.0265851436420661]
				|| Gradient || = 1.5663829126979854
				||X optimum - X|| = 0.9039261225841647
				Rho = 131.15987885248774
				alpha = [0.0390532  0.04065084]
			Iteration 7:
				X = [0.5416832035902011, 0.2858624455754445]
				Gradient = [0.7210366900073373, -1.5116494952172044]
				Hessian = [[0.10012047764576895, 0.11811151332389497], [0.118111513323895, 0.14287100761968696]]
				Gradient Difference = [5.883081373850296, -3.825599947826497]
				|| Gradient || = 1.6748068858012155
				||X optimum - X|| = 0.8485556743731233
				Rho = 4.172363223840358
				alpha = [0.13716952 0.14829232]
			Iteration 8:
				X = [0.6788527215594182, 0.43415477035272887]
				Gradient = [6.604118063857634, -5.337249443043701]
				Hessian = [[0.07038134003768719, 0.09050861891445822], [0.09050861891445822, 0.12112245232137644]]
				Gradient Difference = [-6.457849646597513, 4.681269813788802]
				|| Gradient || = 8.491207630169045
				||X optimum - X|| = 0.6506276956634766
				Rho = 8.535159318372058
				alpha = [-0.03081685 -0.01748417]
			Iteration 9:
				X = [0.6480358752819482, 0.416670597506099]
				Gradient = [0.1462684172601203, -0.6559796292548992]
				Hessian = [[0.30561098614524657, 0.41109057032235746], [0.4110905703223575, 0.5579827437251648]]
				Gradient Difference = [0.1070224001087361, 0.03982096163923643]
				|| Gradient || = 0.6720890743682534
				||X optimum - X|| = 0.6812869710352869
				Rho = 126.75682389944961
				alpha = [0.04907724 0.06621531]
			Iteration 10:
				X = [0.6971131183498686, 0.48288590643730256]
				Gradient = [0.2532908173688564, -0.6161586676156627]
				Hessian = [[0.16230294454626615, 0.23597192260193994], [0.2359719226019397, 0.34682976768906865]]
				Gradient Difference = [3.7390314999233287, -2.1900947915765023]
				|| Gradient || = 0.6661889685676972
				||X optimum - X|| = 0.5992891195716062
				Rho = 14.715023554715968
				alpha = [0.09005494 0.12271638]
			Iteration 11:
				X = [0.7871680618898647, 0.6056022903639074]
				Gradient = [3.992322317292185, -2.806253459192165]
				Hessian = [[0.2292375036049958, 0.36126314087750744], [0.36126314087750777, 0.5744312357772094]]
				Gradient Difference = [-2.6512962200166132, 1.7217584990340817]
				|| Gradient || = 4.879927864464518
				||X optimum - X|| = 0.44815955556688963
				Rho = 62.440198726603775
				alpha = [0.01423136 0.03121626]
			Iteration 12:
				X = [0.8013994182894901, 0.6368185528341034]
				Gradient = [1.3410260972755716, -1.0844949601580833]
				Hessian = [[0.20441446741158645, 0.3459349996815262], [0.3459349996815258, 0.5909973442747029]]
				Gradient Difference = [1.715873362357484, -0.7700139014361895]
				|| Gradient || = 1.7246681745142842
				||X optimum - X|| = 0.4139359305753342
				Rho = 26.229012591453746
				alpha = [0.08437458 0.13850448]
			Iteration 13:
				X = [0.8857739990534244, 0.7753230330913874]
				Gradient = [3.0568994596330556, -1.8545088615942729]
				Hessian = [[0.2835535791425426, 0.5058669267329967], [0.5058669267329976, 0.9075197028095664]]
				Gradient Difference = [-3.05279853849913, 1.7441286489392958]
				|| Gradient || = 3.575449261846202
				||X optimum - X|| = 0.25204626311750966
				Rho = 61.308064662161534
				alpha = [0.01666505 0.0385213 ]
			Iteration 14:
				X = [0.9024390465267691, 0.8138443316328876]
				Gradient = [0.0041009211339260165, -0.11038021265497716]
				Hessian = [[0.32418785072672796, 0.5991426567632844], [0.5991426567632827, 1.1114224057567947]]
				Gradient Difference = [1.4893711815593647, -0.7146230857847571]
				|| Gradient || = 0.11045636649783779
				||X optimum - X|| = 0.21017153115446502
				Rho = 88.27454791264842
				alpha = [0.05467487 0.0980977 ]
			Iteration 15:
				X = [0.957113914609371, 0.9119420290467957]
				Gradient = [1.4934721026932907, -0.8250032984397343]
				Hessian = [[0.4581222267165395, 0.8732983406814673], [0.8732983406814687, 1.6700786624285224]]
				Gradient Difference = [-0.9814168087215939, 0.5264386628320121]
				|| Gradient || = 1.706191479277622
				||X optimum - X|| = 0.09794601864561775
				Rho = 586.1094959181204
				alpha = [0.01012916 0.02212431]
			Iteration 16:
				X = [0.9672430715825612, 0.9340663363464717]
				Gradient = [0.5120552939716968, -0.2985646356077221]
				Hessian = [[0.3908381518419837, 0.7652370564562863], [0.7652370564562863, 1.5053089346773019]]
				Gradient Difference = [-0.0014593188607124885, 0.03492048202200487]
				|| Gradient || = 0.592740639504339
				||X optimum - X|| = 0.07362244468993002
				Rho = 568.6751384988227
				alpha = [0.02615209 0.05144939]
			Iteration 17:
				X = [0.9933951609686458, 0.985515725068031]
				Gradient = [0.5105959751109843, -0.26364415358571724]
				Hessian = [[0.4254162361383014, 0.8466146705117602], [0.8466146705117594, 1.689828768703618]]
				Gradient Difference = [-0.7046336451731581, 0.3566586931395406]
				|| Gradient || = 0.5746446637005047
				||X optimum - X|| = 0.015919111750815947
				Rho = 1546.942073944615
				alpha = [0.00218989 0.00613894]
			Iteration 18:
				X = [0.9955850497601565, 0.9916546640036891]
				Gradient = [-0.19403767006217373, 0.0930145395538233]
				Hessian = [[0.47186787303770605, 0.9374893605099213], [0.9374893605099299, 1.867245852569936]]
				Gradient Difference = [0.19997167843793753, -0.09659939928630148]
				|| Gradient || = 0.21517974340436125
				||X optimum - X|| = 0.00944120853024745
				Rho = 13470.605310117702
				alpha = [0.0037993  0.00709649]
			Iteration 19:
				X = [0.9993843512698888, 0.9987511572644746]
				Gradient = [0.005934008375763808, -0.0035848597324781802]
				Hessian = [[0.49855408445404337, 0.9968564990079685], [0.9968564990079675, 1.998161969073387]]
				Gradient Difference = [-0.002762710196736334, 0.001944171906321069]
				|| Gradient || = 0.00693279703331769
				||X optimum - X|| = 0.0013923474914553824
				Rho = 1540094.7148819317
				alpha = [0.0005607  0.00113074]
			Iteration 20:
				X = [0.9999450512171479, 0.9998819020145341]
				Gradient = [0.003171298179027474, -0.0016406878261571113]
				Hessian = [[0.49950187754352243, 0.9989816627048766], [0.9989816627048806, 2.002918765016711]]
				Gradient Difference = [-0.003131091222111785, 0.0016201024170902766]
				|| Gradient || = 0.0035705725987862395
				||X optimum - X|| = 0.00013025552928006652
				Rho = 52460881.87974503
				alpha = [5.44666622e-05 1.17030817e-04]
			Iteration 21:
				X = [0.9999995178793201, 0.9999989328318273]
				Gradient = [4.020695691568923e-05, -2.0585409066834666e-05]
				Hessian = [[0.4997949038644785, 0.9995989150065561], [0.9995989150065536, 2.0042156075526085]]
				Gradient Difference = [-4.019728930510211e-05, 2.0579650240086016e-05]
				|| Gradient || = 4.517032710606609e-05
				||X optimum - X|| = 1.1710201786840746e-06
				Rho = 387547556844.8438
				alpha = [4.80995707e-07 1.06488943e-06]
			Iteration 22:
				X = [0.9999999988750274, 0.9999999977212608]
				Gradient = [9.667610587120734e-09, -5.758826748652036e-09]
				Hessian = [[0.49996651893610183, 0.9999360725456049], [0.9999360725456101, 2.004877232472565]]
				Gradient Difference = [-9.67392664599909e-09, 5.761702226273759e-09]
				|| Gradient || = 1.1252856525576031e-08
				||X optimum - X|| = 2.5413019476684327e-09
				Rho = 5.329957141730435e+17
				alpha = [9.24694466e-10 1.87819740e-09]
Running: Quadratic
===================
	- Starting Point : [-0.2  1.2]
	------------------------------
		- Quasi Newton:
			- BFGS Method:
			---------------
			Iteration 0:
				X = [-0.2, 1.2]
				Gradient = [-86.09999999986684, 15.600000000226544]
				Hessian = [[0.016029481342573586, 0.11614654634893293], [0.11614654634893293, 0.9902137232096606]]
				Gradient Difference = [110.44725907932707, -13.00246148983053]
				|| Gradient || = 87.50182855223163
				||X optimum - X|| = 4.368065933568311
				Rho = 0.034067175646763
				alpha = [ 0.26022128 -0.04714811]
			Iteration 1:
				X = [0.060221282671646836, 1.1528518930343448]
				Gradient = [24.347259079460226, 2.5975385103960136]
				Hessian = [[0.001663538059995039, 0.0014497433738011982], [0.0014497433738011913, 0.0749478593216179]]
				Gradient Difference = [-19.748764166840903, -3.5789649113077138]
				|| Gradient || = 24.485428952651592
				||X optimum - X|| = 4.104987677056572
				Rho = 0.5513453930245763
				alpha = [-0.0380414 -0.2968664]
			Iteration 2:
				X = [0.02217988117700543, 0.8559854943530075]
				Gradient = [4.598494912619322, -0.9814264009117002]
				Hessian = [[0.0031066626987237733, -0.008889846304421876], [-0.008889846304421876, 0.13056192972881464]]
				Gradient Difference = [-0.6686481858787783, 0.466789904403353]
				|| Gradient || = 4.702058404761927
				||X optimum - X|| = 4.068877494377994
				Rho = 28.259099718363586
				alpha = [-0.00622695  0.06688917]
			Iteration 3:
				X = [0.015952926292764923, 0.922874664654031]
				Gradient = [3.9298467267405437, -0.5146364965083472]
				Hessian = [[0.002950248255175366, -0.006398512454225715], [-0.006398512454225715, 0.10603425312580259]]
				Gradient Difference = [-4.142123153703762, 0.7132057677440486]
				|| Gradient || = 3.9634008148573305
				||X optimum - X|| = 4.08953893882617
				Rho = 7.024524461416287
				alpha = [-0.01678375  0.10212767]
			Iteration 4:
				X = [-0.0008308213015069954, 1.025002332147696]
				Gradient = [-0.21227642696321908, 0.19856927123570145]
				Hessian = [[0.0023799431800383825, -0.0034950198298086075], [-0.0034950198298086092, 0.11704618861845432]]
				Gradient Difference = [0.5394437546051809, -0.1753840202211172]
				|| Gradient || = 0.29067341970560095
				||X optimum - X|| = 4.130045646428658
				Rho = 201.84985677468282
				alpha = [ 0.00189682 -0.0224134 ]
			Iteration 5:
				X = [0.0010659948117842624, 1.0025889344168208]
				Gradient = [0.3271673276419618, 0.02318525101458423]
				Hessian = [[0.0033177608070651303, -0.000916740849355376], [-0.000916740849355376, 0.1239016458488668]]
				Gradient Difference = [-0.21420415182008792, -0.014258556854644411]
				|| Gradient || = 0.3279878292574163
				||X optimum - X|| = 4.122700298259106
				Rho = 5820.033591752192
				alpha = [-0.00069761 -0.00157029]
			Iteration 6:
				X = [0.0003683880736864545, 1.0010186454511758]
				Gradient = [0.1129631758218739, 0.008926694159939819]
				Hessian = [[0.0033254581362749036, -0.0008142007452909489], [-0.0008142007452909472, 0.12440488221770653]]
				Gradient Difference = [-0.11239390004683053, -0.008793749235074349]
				|| Gradient || = 0.11331533418028074
				||X optimum - X|| = 4.122995435318985
				Rho = 19992.299088102627
				alpha = [-0.0003666  -0.00100247]
			Iteration 7:
				X = [1.786741489152833e-06, 1.0000161713105187]
				Gradient = [0.0005692757750433777, 0.00013294492486547033]
				Hessian = [[0.003323193592790803, -0.0007901507660796562], [-0.0007901507660796562, 0.12502179559431242]]
				Gradient Difference = [-0.0005685184220740136, -0.00013217461921094192]
				|| Gradient || = 0.0005845932441439295
				||X optimum - X|| = 4.123107814374229
				Rho = 318522328.89575845
				alpha = [-1.7848589e-06 -1.6075493e-05]
			Iteration 8:
				X = [1.8825881587459176e-09, 1.0000000958175597]
				Gradient = [7.573529693641603e-07, 7.703056545284239e-07]
				Hessian = [[0.00332185686835641, -0.0008219266851836509], [-0.0008219266851836509, 0.12519699541063334]]
				Gradient Difference = [-7.648192206275539e-07, -7.694689322812055e-07]
				|| Gradient || = 1.0802565998887369e-06
				||X optimum - X|| = 4.123105647030454
				Rho = 13315110819995.553
				alpha = [-1.90817293e-09 -9.57065731e-08]
			Iteration 9:
				X = [-2.558477348502419e-11, 1.0000000001109866]
				Gradient = [-7.466251263393606e-09, 8.367222472184019e-10]
				Hessian = [[0.0033325953560724205, -0.0008398982814488091], [-0.0008398982814488073, 0.12515012149440116]]
				Gradient Difference = [7.43782404140609e-09, -8.361542571193774e-10]
				|| Gradient || = 7.51298955457259e-09
				||X optimum - X|| = 4.1231056256694
				Rho = 3.542212897850291e+18
				alpha = [ 2.54895424e-11 -1.10891822e-10]
	- Starting Point : [3.8 0.1]
	------------------------------
		- Quasi Newton:
			- BFGS Method:
			---------------
			Iteration 0:
				X = [3.8, 0.1]
				Gradient = [11.500000001163357, 433.5999999991458]
				Hessian = [[0.9994608444637588, -0.02341499164311975], [-0.02341499164311975, 0.0007791530557724959]]
				Gradient Difference = [-11.446675466264544, -488.46988849907126]
				|| Gradient || = 433.7524754964356
				||X optimum - X|| = 0.22360679774997913
				Rho = 0.018174964656755135
				alpha = [-0.00298557 -0.112569  ]
			Iteration 1:
				X = [3.7970144293060404, -0.012568995892813503]
				Gradient = [0.05332453489881317, -54.86988849992547]
				Hessian = [[0.3979768448457206, -0.004885477877555824], [-0.004885477877555824, 0.00021092211347500332]]
				Gradient Difference = [-0.3669039006504171, 59.85919126477724]
				|| Gradient || = 54.869914411271644
				||X optimum - X|| = 0.20337433851817696
				Rho = 0.9766290982448681
				alpha = [-0.43846001  0.01441813]
			Iteration 2:
				X = [3.3585544178711477, 0.00184913212947432]
				Gradient = [-0.31357936575160394, 4.989302764851766]
				Hessian = [[0.9451382225122684, -0.00992479779969958], [-0.00992479779969958, 0.00022130535551823457]]
				Gradient Difference = [0.06654119677484971, -8.693560127909922]
				|| Gradient || = 4.999147337094863
				||X optimum - X|| = 0.6414482474231685
				Rho = 30.87065454716642
				alpha = [ 0.14917245 -0.00258434]
			Iteration 3:
				X = [3.5077268727438047, -0.0007352072086923398]
				Gradient = [-0.24703816897675424, -3.7042573630581557]
				Hessian = [[1.4829317706111085, -0.007112423058385357], [-0.007112423058385357, 0.00013762711745032554]]
				Gradient Difference = [0.10075545154100696, -6.65142526705756]
				|| Gradient || = 3.7124857533331963
				||X optimum - X|| = 0.4922736762698509
				Rho = 32.59869723578588
				alpha = [ 0.19672121 -0.00163203]
			Iteration 4:
				X = [3.704448083336779, -0.00236723909193202]
				Gradient = [-0.14628271743574728, -10.355682630115716]
				Hessian = [[1.9871507964858628, 0.002236716923570372], [0.002236716923570372, 0.00018420917879577353]]
				Gradient Difference = [0.07071507048244524, 1.2302784457426608]
				|| Gradient || = 10.356715761721992
				||X optimum - X|| = 0.29556139677607074
				Rho = 94.29523330397913
				alpha = [0.14327329 0.0003848 ]
			Iteration 5:
				X = [3.8477213765898206, -0.0019824409148520667]
				Gradient = [-0.07556764695330204, -9.125404184373055]
				Hessian = [[1.8215354242928914, 0.0019609402467912], [0.0019609402467912, 0.0001977083766672916]]
				Gradient Difference = [0.0844722374872885, 8.519424740444222]
				|| Gradient || = 9.12571706757553
				||X optimum - X|| = 0.15229152707777316
				Rho = 33.14565610315158
				alpha = [0.17057526 0.00185001]
			Iteration 6:
				X = [4.018296632395244, -0.00013243426905434896]
				Gradient = [0.008904590533986465, -0.6059794439288325]
				Hessian = [[2.040346962525243, -0.00015713832878672292], [-0.00015713832878672292, 0.00021640728532186407]]
				Gradient Difference = [-0.007331232735098518, 0.46760821560688837]
				|| Gradient || = 0.6060448648383014
				||X optimum - X|| = 0.018297111680325766
				Rho = 6326.754464288015
				alpha = [-0.01503174  0.00010235]
			Iteration 7:
				X = [4.003264894779092, -3.008842686072144e-05]
				Gradient = [0.001573357798887947, -0.13837122832194415]
				Hessian = [[2.0596816696637927, -0.00018489163236818512], [-0.0001848916323681834, 0.00021523852540475424]]
				Gradient Difference = [-0.001556673311586083, 0.13893405363666794]
				|| Gradient || = 0.13838017300931127
				||X optimum - X|| = 0.003265033419733276
				Rho = 108392.39538154202
				alpha = [-3.23193923e-03  3.01917767e-05]
			Iteration 8:
				X = [4.000032955549595, 1.0334984218313497e-07]
				Gradient = [1.6684487301863847e-05, 0.0005628253147237847]
				Hessian = [[2.0066202817497993, -0.0009230941318772497], [-0.0009230941318772497, 0.00021078110848003908]]
				Gradient Difference = [-1.7366461783970918e-05, -0.0006361468949485974]
				|| Gradient || = 0.000563072559276736
				||X optimum - X|| = 3.2955711649133664e-05
				Rho = 1492340984.641727
				alpha = [-3.42606710e-05 -1.18056869e-07]
			Iteration 9:
				X = [3.999998694878623, -1.47070265260113e-08]
				Gradient = [-6.819744821070694e-07, -7.332158022481272e-05]
				Hessian = [[2.0149273901483147, -0.0009400441576159109], [-0.0009400441576159092, 0.0002093156787126198]]
				Gradient Difference = [6.800408484259751e-07, 7.388144946612502e-05]
				|| Gradient || = 7.332475172721619e-05
				||X optimum - X|| = 1.3052042391271892e-06
				Rho = 505076711844.3597
				alpha = [1.30078111e-06 1.48252773e-08]
			Iteration 10:
				X = [3.9999999956597296, 1.1825078675977688e-10]
				Gradient = [-1.9336336810942954e-09, 5.598692413123059e-07]
				Hessian = [[2.007263301641291, -0.000819567717882233], [-0.0008195677178822339, 0.0002083815498197204]]
				Gradient Difference = [1.9732024737873682e-09, -5.63341324285592e-07]
				|| Gradient || = 5.598725804206075e-07
				||X optimum - X|| = 4.341880939495625e-09
				Rho = 1.3198187422744694e+16
				alpha = [ 4.42243328e-09 -1.19007111e-10]
	- Starting Point : [1.9 0.6]
	------------------------------
		- Quasi Newton:
			- BFGS Method:
			---------------
			Iteration 0:
				X = [1.9, 0.6]
				Gradient = [205.34999994481495, 650.3999999978305]
				Hessian = [[0.9581327114130217, -0.21629357783159311], [-0.21629357783159311, 0.04987463917982304]]
				Gradient Difference = [-186.15650061626354, -823.4568195746306]
				|| Gradient || = 682.0474928291383
				||X optimum - X|| = 2.1840329667841556
				Rho = 0.001407787670444056
				alpha = [-0.25421099 -0.8051562 ]
			Iteration 1:
				X = [1.6457890130010624, -0.20515620154852698]
				Gradient = [19.19349932855141, -173.05681957680008]
				Hessian = [[0.022223110265515343, -0.002124016401956816], [-0.0021240164019568223, 0.0008650131178513744]]
				Gradient Difference = [-20.704046790331887, 162.1254199044575]
				|| Gradient || = 174.11792905531652
				||X optimum - X|| = 2.363133182522799
				Rho = 0.021495279729334633
				alpha = [-0.80446537  0.18421635]
			Iteration 2:
				X = [0.8413236471859242, -0.02093985162446252]
				Gradient = [-1.510547461780476, -10.931399672342579]
				Hessian = [[0.048508272083526226, 0.009231728827453711], [0.009231728827453711, 0.004997327047325445]]
				Gradient Difference = [-0.03784539459594782, 1.3200566795212865]
				|| Gradient || = 11.03527310177609
				||X optimum - X|| = 3.158745760458237
				Rho = 127.30470966538368
				alpha = [0.01035059 0.00624738]
			Iteration 3:
				X = [0.8516742377899651, -0.01469247509616595]
				Gradient = [-1.5483928563764238, -9.611342992821292]
				Hessian = [[0.04813611780887017, 0.00657126425686453], [0.00657126425686453, 0.0030270688035086385]]
				Gradient Difference = [0.8426425508467617, 18.76010889678703]
				|| Gradient || = 9.735267565061193
				||X optimum - X|| = 3.148360045137762
				Rho = 0.7649420705531559
				alpha = [0.16383917 0.06232537]
			Iteration 4:
				X = [1.0155134119366345, 0.047632892171415406]
				Gradient = [-0.7057503055296621, 9.148765903965739]
				Hessian = [[0.035151280133705484, 0.0007826676805050489], [0.0007826676805050489, 0.0029149165598690713]]
				Gradient Difference = [-0.5711369281957701, -7.7564009886010865]
				|| Gradient || = 9.175946875354132
				||X optimum - X|| = 2.98486667821977
				Rho = 5.160833782406167
				alpha = [-0.02614688 -0.02305627]
			Iteration 5:
				X = [0.9893665334081054, 0.0245766200699155]
				Gradient = [-1.2768872337254322, 1.3923649153646522]
				Hessian = [[60.433403549452755, -4.751925950504505], [-4.751925950504505, 0.3721889005250697]]
				Gradient Difference = [-0.019992317135120174, -0.2634717468641412]
				|| Gradient || = 1.8892117576358138
				||X optimum - X|| = 3.010733777738774
				Rho = -14382.892449851213
				alpha = [ 0.04379446 -0.00305925]
			Iteration 6:
				X = [1.0331609952412153, 0.02151737089022477]
				Gradient = [-1.2968795508605524, 1.128893168500511]
				Hessian = [[0.06816537495428895, -0.020892507662343667], [-0.02089250766234373, 0.0014047816978878065]]
				Gradient Difference = [0.43085986889401795, -28.168804768302635]
				|| Gradient || = 1.7193883666372154
				||X optimum - X|| = 2.9669170324443055
				Rho = 0.6118231269699596
				alpha = [ 0.61788669 -0.04857276]
			Iteration 7:
				X = [1.6510476892179575, -0.027055393611926222]
				Gradient = [-0.8660196819665344, -27.039911599802124]
				Hessian = [[0.24012728199156064, -0.011369115609497205], [-0.011369115609497205, 0.0003900046655947941]]
				Gradient Difference = [-0.10874986178244228, 3.6754667307459243]
				|| Gradient || = 27.05377625054711
				||X optimum - X|| = 2.349108118553038
				Rho = 58.14938434465411
				alpha = [-0.06790061  0.00266984]
			Iteration 8:
				X = [1.5831470743104705, -0.024385554687575438]
				Gradient = [-0.9747695437489767, -23.3644448690562]
				Hessian = [[nan, nan], [nan, nan]]
				Gradient Difference = [0.0, 0.0]
				|| Gradient || = 23.384769823596454
				||X optimum - X|| = 2.416975945203315
				Rho = inf
				alpha = [-2.40566042e-17 -1.50144607e-18]
			Iteration 9:
				X = [1.5831470743104705, -0.024385554687575438]
				Gradient = [-0.9747695437489767, -23.3644448690562]
				Hessian = [[nan, nan], [nan, nan]]
				Gradient Difference = [nan, nan]
				|| Gradient || = 23.384769823596454
				||X optimum - X|| = 2.416975945203315
				Rho = nan
				alpha = [nan nan]
			Iteration 10:
				X = [nan, nan]
				Gradient = [nan, nan]
				Hessian = [[nan, nan], [nan, nan]]
				Gradient Difference = [nan, nan]
				|| Gradient || = nan
				||X optimum - X|| = nan
				Rho = nan
				alpha = [nan nan]
